#+TITLE: Extend a Data Format: jsonic

A JSON contains one of 6 values:
1. a null
2. a Boolean
3. a number
4. a string
5. an array
6. an object

We wish to bring automation features into JSON. This hybrid will be a domain specific language based on JSON.

* Specification

** Ground Rules

1. Every jsonic program produces valid JSON.
2. Every valid JSON file is a valid jsonic program.
3. Racket expressions can be embedded anywhere within a jsonic program that a JSON value would appear
4. If the characters // appear in a line, the rest of the line will be commented out

#+BEGIN_SRC racket :tangle jsonic-demo-json.rkt
#lang jsonic-demo
[
 null,
 42,
 true,
 ["array", "of", "strings"],
 {
  "key-1": null,
  "key-2": false,
  "key-3": {"subkey": 21}
  }
 ]
#+END_SRC


#+BEGIN_SRC racket :tangle jsonic-demo-racket.rkt
#lang jsonic-demo
// a line comment
[
  @$ 'null $@,
  @$ (* 6 7) $@,
  @$ (= 2 (+ 1 1)) $@,
  @$ (list "array" "of" "strings") $@,
  @$ (hash 'key-1 'null
           'key-2 (even? 3)
           'key-3 (hash 'subkey 21)) $@
]
#+END_SRC

** Ready to Roll

Our language will have two main parts:
1. A reader (comprising a tokenizer and parser)
2. An expander

* Setup

#+BEGIN_SRC bash
cd path/to/jsonic
raco pkg install
#+END_SRC

* The Reader

** Booting the Reader

When we invoke #lang jsonic, Racket will look for a "main.rkt" module within our jsonic directory.
Within that module, it will expect to find a reader submodule that provides our read-syntax function.

#+BEGIN_SRC racket :tangle main.rkt
#lang br/quicklang
(module reader br
  (require "reader.rkt")
  (provide read-syntax))
#+END_SRC

** The Reader

A recap:

The reader converts the source code of our language from a string of characters into Racket-style parenthesized forms, also known as sexprs

By convention, Racket expects the name of the main reader function to be read-syntax. This read-syntax function must return one value: code for a module expression, packaged as a syntax object. This syntax object must have no bindings. In bf, we made a read-syntax function by relying on a tokenizer and parser to do the heavy lifting.

#+BEGIN_SRC racket
#lang br/quicklang
(require "tokenizer.rkt" "parser.rkt")

(define (read-syntax path port)
  (define parse-tree (parse path (make-tokenizer port)))
  (define module-datum `(module jsonic-module jsonic/expander ,parse-tree))
  (datum->syntax #f module-datum))
(provide read-syntax)
#+END_SRC

* The Tokenizer

#+BEGIN_SRC racket
#lang br/quicklang
(require brag/support)

(define (make-tokenizer port)
  (define (next-token)
    (define jsonic-lexer
      (lexer
       ; add rules
       [(from/to "//" "\n") (next-token)]  ; handles line comments. ignore everything from // to the new line. once we have the match, ignore it by calling next-token
       [(from/to "@$" "$@") (token 'SEXP-TOK (trim-ends "@$" lexeme "$@"))]  ; takes in sexpression tokens
       [any-char (token 'CHAR-TOK lexeme)]  ; handles tokens not processed by the above
       ))
    (jsonic-lexer port))
  next-token)
(provide make-tokenizer)
#+END_SRC

Within next-token, we use a helper function called a lexer to break down the source code into tokens.

- The lexer must be able to process every token that might appear in the source, including eof
- The lexer consists of a series of branches, each representing a lexing rule. On the left side of the branch is a pattern
  that works like a regular expression. On the right is a token-creating expression
- Each time next-token is called, jsonic-lexer will read as many characters from the input port as it can while still matching
  a rule pattern
- The lexer rule will convert the matched characters (known as the lexeme) into a token using the expression on the right
- This token will be returned as the result. The process repeats until the lexer gets the eof signal

We can even do some repl testing:

First run the tokenizer (F5 in block)

#+BEGIN_SRC racket
(apply-tokenizer-maker make-tokenizer "// comment\n")
(apply-tokenizer-maker make-tokenizer "@$ (+ 6 7) $@")
(apply-tokenizer-maker make-tokenizer "hi")
#+END_SRC

* The Parser

The parser will take the tokens generated by the tokenizer and organize them into a parse tree. Later, this parse tree will be passed to the expander for further processing.

#+BEGIN_SRC racket :tangle parser.rkt
#lang brag
jsonic-program : (jsonic-char | jsonic-sexp)*
jsonic-char    : CHAR-TOK
jsonic-sexp    : SEXP-TOK
#+END_SRC

#+BEGIN_SRC racket :tangle jsonic-parser-test.rkt
#lang br
(require jsonic/parser jsonic/tokenizer brag/support)

; test comment
(parse-to-datum (apply-tokenizer-maker make-tokenizer "// line comment\n"))

; a program with a single sexp between delimiters
(parse-to-datum (apply-tokenizer-maker make-tokenizer "@$ 42 $@"))

; a program without nested delimiters
(parse-to-datum (apply-tokenizer-maker make-tokenizer "hi"))

; a program that contains all of the above
(parse-to-datum (apply-tokenizer-maker make-tokenizer "hi\n// comment\n@$ 42 $@"))
#+END_SRC

** Here Strings

A here string is introduced with #<<LABEL, where LABEL is an arbitrary name that will terminate the here string.

#+BEGIN_SRC racket :tangle jsonic-parser-test.rkt
; try a multiline program with a here string
(parse-to-datum (apply-tokenizer-maker make-tokenizer #<<GOGOGO
"foo"
//comment
@$ 42 $@
GOGOGO
                                       ))
#+END_SRC

* The Expander

The expander determines how the code produced by the reader corresponds to real Racket expressions, which are then evaluated to produce a result.
It works by adding bindings to identifiers in the code. The main job of the expander is to bind the identifiers in a parse tree, even though no define expressions
appear in the parse tree itself. This is why the reader delivers code without bindings, so the expander can start with a blank slate. The expander does this
job by exporting a binding (using provide) for each identifier in the parse tree.

** Designing the Expander

Racket starts the expander for a language by invoking a macro called #%module-begin.

We also just saw how the parse tree produced by the jsonic parser follows the production rules of the grammar:

#+BEGIN_SRC racket
'(jsonic-program
  (jsonic-char "\"")
  (jsonic-char "f")
  (jsonic-char "o")
  (jsonic-char "o")
  (jsonic-char "\"")
  (jsonic-char "\n")
  (jsonic-sexp " 42 "))
#+END_SRC

In turn, we can use these production rules to organize the rest of our expander.

1. Each production rule in the grammar will have a corresponding macro or function in the expander
2. The name (on the left side) of each rule is the name of the corresponding macro or function
3. The pattern (on the right side) of each rule describes the possible input to that corresponding macro or function

We should have 3 transformers:
- jsonic-program, that accepts any # of arguments
- jsonic-char that accepts 1 argument, which is the string value from a CHAR-TOK token
- jsonic-sexp that accepts 1 argument, which is the string value from a SEXP-TOK token

** Starting the Expander

#+BEGIN_SRC racket :tangle expander.rkt
#lang br/quicklang
(require json)

(define-macro (jsonic-mb PARSE-TREE)
  #'(#%module-begin
     ; validate our new string as valid JSON, and return the result
     (define result-string PARSE-TREE)
     (define validated-jsexpr (string->jsexpr result-string))
     (display result-string)))
(provide (rename-out [jsonic-mb #%module-begin]))
#+END_SRC

** Expanding the Parse Tree

Now we'll add the macros that correspond to the production rules in our jsonic grammar

#+BEGIN_SRC racket :tangle expander.rkt
(define-macro (jsonic-char CHAR-TOK-VALUE)
  #'CHAR-TOK-VALUE)
(provide jsonic-char)

(define-macro (jsonic-program SEXP-OR-JSON-STR ...)
  #'(string-trim (string-append SEXP-OR-JSON-STR ...)))
(provide jsonic-program)

(define-macro (jsonic-sexp SEXP-STR)
  (with-pattern ([SEXP-DATUM (format-datum '~a #'SEXP-STR)])
    #'(jsexpr->string SEXP-DATUM)))
(provide jsonic-sexp)
#+END_SRC

* Testing the Language

#+BEGIN_SRC racket :tangle jsonic-lang-test.rkt
#lang jsonic

; test passing valid json returns the same json
[
 null,
 42,
 true,
 ["array", "of", "strings"],
 {
  "key-1": null,
  "key-2": false,
  "key-3": {"subkey": 21}
 }
 ]

; test invalid json fails (3/5 not allowed)
[
 null,
 3/5,
 true,
 ["array", "of", "strings"],
 {
  "key-1": null,
  "key-2": false,
  "key-3": {"subkey": 21}
 }
 ]

; test replacing json with racket sexprs
// a line comment
[
 @$ 'null $@,
 @$ (* 6 7) $@,
 @$ (= 2 (+ 1 1)) $@,
 @$ (list "array" "of" "strings") $@,
 @$ (hash 'key-1 'null
          'key-2 (even? 3)
          'key-3 (hash 'subkey 21)) $@

]

; test racket sexprs fail when translating to invalid json
// a line comment
[
 @$ 'null $@,
 @$ (/ 3 5) $@,
 @$ (= 2 (+ 1 1)) $@,
 @$ (list "array" "of" "strings") $@,
 @$ (hash 'key-1 'null
          'key-2 (even? 3)
          'key-3 (hash 'subkey 21)) $@

]
#+END_SRC

* Contracts

A contract wraps around a function and ensures its input arguments and return value meet requirements that we specify. If the function tries to accept arguments or return a value that doesn't meet these requirements, the contract raises an error.

#+BEGIN_SRC racket :tangle contract-test.rkt
#lang racket
(module our-submod br
  (require racket/contract)
  (define (our-div num denom)
    (/ num denom))
  ; contract-out lets us attach a contract to an exported function
  (provide (contract-out
            ; (number? (not/c zero?) . -> . number?) means input must be a number not zero and output must be a number
            [out-div (number? (not/c zero?) . -> . number?)])))

(require (submod "." our-submod))
(our-div 42 6)
#+END_SRC

** Adding contracts to our language

#+BEGIN_SRC racket :tangle reader.rkt
#lang br/quicklang
(require "tokenizer.rkt" "parser.rkt" racket/contract)

; takes any for path and a port for port and returns syntax
(define (read-syntax path port)
  (define parse-tree (parse path (make-tokenizer port)))
  (define module-datum `(module jsonic-module jsonic/expander ,parse-tree))
  (datum->syntax #f module-datum))

(provide (contract-out
          [read-syntax (any/c input-port? . -> . syntax?)]))
#+END_SRC

Next we can add a contract to make-tokenizer:

#+BEGIN_SRC racket
#lang br/quicklang
(require brag/support racket/contract)

(define (jsonic-token? x)
  (or (eof-object? x)
      (token-struct? x)))

; make-tokenizer takes an input-port
(define (make-tokenizer port)
  ; next-token takes nothing and returns either EOF or a token-structure
  (define (next-token)
    (define jsonic-lexer
      (lexer
       ; add rules
       [(from/to "//" "\n") (next-token)]  ; handles line comments. ignore everything from // to the new line. once we have the match, ignore it by calling next-token
       [(from/to "@$" "$@") (token 'SEXP-TOK (trim-ends "@$" lexeme "$@"))]  ; takes in sexpression tokens
       [any-char (token 'CHAR-TOK lexeme)]  ; handles tokens not processed by the above
       ))
    (jsonic-lexer port))
  next-token)

(provide (contract-out
          [make-tokenizer (input-port? . -> . (-> jsonic-token?))]))
#+END_SRC

* Unit Tests

Racketeers often like to write unit tests for a function before they start writing that function. It's also common to put the unit tests near the function definition, so they can be worked in tandem.

** Writing Tests with Rackunit

The rackunit library helps us create unit tests called checks.

#+BEGIN_SRC racket :tangle rackunit-test.rkt
#lang racket

(module+ test
  (require rackunit))

(define (our-div num denom)
  (/ num denom))

(module+ test
  (check-equal? (our-div 42 6) 7)
  (check-equal? (our-div 42 2) 21)
  (check-exn exn:fail? (lambda () (our-div 42 0))))

(define (our-mult factor1 factor2)
  (* factor1 factor2))

(module+ test
  (check-equal? (our-mult 6 7) 42)
  (check-exn exn:fail? (lambda () (our-mult "a" "b"))))
#+END_SRC

** Adding Tests to the Tokenizer

#+BEGIN_SRC racket
#lang br/quicklang
(require brag/support racket/contract)

(module+ test
  (require rackunit))

(define (jsonic-token? x)
  (or (eof-object? x)
      (token-struct? x)))

(module+ test
  (check-true (jsonic-token? eof))
  (check-true (jsonic-token? (token 'A-TOKEN-STRUCT "hi")))
  (check-false (jsonic-token? 42)))

; make-tokenizer takes an input-port
(define (make-tokenizer port)
  ; next-token takes nothing and returns either EOF or a token-structure
  (define (next-token)
    (define jsonic-lexer
      (lexer
       ; add rules
       [(from/to "//" "\n") (next-token)]  ; handles line comments. ignore everything from // to the new line. once we have the match, ignore it by calling next-token
       [(from/to "@$" "$@") (token 'SEXP-TOK (trim-ends "@$" lexeme "$@"))]  ; takes in sexpression tokens
       [any-char (token 'CHAR-TOK lexeme)]  ; handles tokens not processed by the above
       ))
    (jsonic-lexer port))
  next-token)

(provide (contract-out
          [make-tokenizer (input-port? . -> . (-> jsonic-token?))]))

(module+ test
  (check-equal?
   (apply-tokenizer-maker make-tokenizer "// comment\n")
   empty)
  (check-equal?
   (apply-tokenizer-maker make-tokenizer "@$ (+ 6 7) $@")
   (list (token-struct 'SEXP-TOK " (+ 6 7) " #f #f #f #f #f)))
  (check-equal?
   (apply-tokenizer-maker make-tokenizer "hi")
   (list (token-struct 'CHAR-TOK "h" #f #f #f #f #f)
         (token-struct 'CHAR-TOK "i" #f #f #f #f #f))))
#+END_SRC

** Adding Parser Tests in a Separate Module

One limitation of the test submodule is that we can only use it in modules that use a language that supports module+.
Our parser.rkt module is written in #lang brag, which doesn't support module+. In this case, we can still make unit
tests for parse using rackunit, but we'll have to move them to a separate file.

Recall that our parser takes a list of tokens and turns them into a parse tree.

#+BEGIN_SRC racket :tangle parser-test.rkt
#lang br
(require jsonic/parser
         jsonic/tokenizer
         brag/support
         rackunit)

(check-equal?
 (parse-to-datum
  (apply-tokenizer-maker make-tokenizer "// line comment\n"))
 '(jsonic-program))

(check-equal?
 (parse-to-datum
  (apply-tokenizer-maker make-tokenizer "@$ 42 $@"))
 '(jsonic-program (jsonic-sexp " 42 ")))

(check-equal?
 (parse-to-datum
  (apply-tokenizer-maker make-tokenizer "hi"))
 '(jsonic-program
   (jsonic-char "h")
   (jsonic-char "i")))

(check-equal?
 (parse-to-datum
  (apply-tokenizer-maker make-tokenizer
                         "hi\n//comment \n@$ 42 $@"))
 '(jsonic-program
   (jsonic-char "h")
   (jsonic-char "i")
   (jsonic-char "\n")
   (jsonic-sexp " 42 ")))
#+END_SRC

* Source Locations

Source locations are typically tracked with a set of five fields, which can each be #f if the value is unknown, or a number:

- The source origin, which is usually a path in the filesystem
- A position (counting from 1). Represents the number of characters away from the start of the file
- A line number (counting from 1), which is the vertical line number from the top
- A column number (counting from 0), which is the horizontal offset within the current line
- A span (counting from 0), which is the number of characters that the code occupies relative to its position measurement

Racket has a standard structure type called a srcloc that holds all five of these values.

#+BEGIN_SRC racket :tangle srcloc-test.rkt
#lang racket

(define stx #'foobar)
(syntax-position stx)
(syntax-line stx)
(syntax-column stx)
(syntax-span stx)
#+END_SRC

** Gathering the Source-Location Data

For complete source locations, we need four pieces of data: position, line, column, and span.
We'll collect this data, then embed it within the token structures emitted by make-tokenizer.

Line count and column count are available from the input port. We can add source-location data to our SEXP-TOK and CHAR-TOK rules.


#+BEGIN_SRC racket :tangle tokenizer.rkt
#lang br/quicklang
(require brag/support racket/contract)

(module+ test
  (require rackunit))

(define (jsonic-token? x)
  (or (eof-object? x)
      (token-struct? x)))

(module+ test
  (check-true (jsonic-token? eof))
  (check-true (jsonic-token? (token 'A-TOKEN-STRUCT "hi")))
  (check-false (jsonic-token? 42)))

; make-tokenizer takes an input-port
(define (make-tokenizer port)
  (port-count-lines! port)  ; turn on line and column counting
  ; next-token takes nothing and returns either EOF or a token-structure
  (define (next-token)
    (define jsonic-lexer
      (lexer
       ; add rules
       [(from/to "//" "\n") (next-token)]  ; handles line comments. ignore everything from // to the new line. once we have the match, ignore it by calling next-token
       [(from/to "@$" "$@")
        (token 'SEXP-TOK (trim-ends "@$" lexeme "$@")  ; takes in sexpression tokens
               #:position (+ (pos lexeme-start) 2)  ; adjust to account for delimiter spacing
               #:line (line lexeme-start)
               #:column (+ (col lexeme-start) 2)
               #:span (- (pos lexeme-end)
                         (pos lexeme-start) 4))]
       [any-char (token 'CHAR-TOK lexeme ; handles tokens not processed by the above
                        #:position (pos lexeme-start)
                        #:line (line lexeme-start)
                        #:column (col lexeme-start)
                        #:span (- (pos lexeme-end)
                                  (pos lexeme-start)))]))
    (jsonic-lexer port))
  next-token)

(provide (contract-out
          [make-tokenizer (input-port? . -> . (-> jsonic-token?))]))

(module+ test
  (check-equal?
   (apply-tokenizer-maker make-tokenizer "// comment\n")
   empty)
  (check-equal?
   (apply-tokenizer-maker make-tokenizer "@$ (+ 6 7) $@")
   (list (token'SEXP-TOK " (+ 6 7) "
                         #:position 3
                         #:line 1
                         #:column 2
                         #:span 9)))
  (check-equal?
   (apply-tokenizer-maker make-tokenizer "hi")
   (list (token 'CHAR-TOK "h"
                       #:position 1
                       #:line 1
                       #:column 0
                       #:span 1)
         (token 'CHAR-TOK "i"
                #:position 2
                #:line 1
                #:column 1
                #:span 1))))
#+END_SRC

* DrRacket Integration
