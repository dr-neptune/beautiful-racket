#+TITLE: Extend a Data Format: jsonic

A JSON contains one of 6 values:
1. a null
2. a Boolean
3. a number
4. a string
5. an array
6. an object

We wish to bring automation features into JSON. This hybrid will be a domain specific language based on JSON.

* Specification

** Ground Rules

1. Every jsonic program produces valid JSON.
2. Every valid JSON file is a valid jsonic program.
3. Racket expressions can be embedded anywhere within a jsonic program that a JSON value would appear
4. If the characters // appear in a line, the rest of the line will be commented out

#+BEGIN_SRC racket :tangle jsonic-demo-json.rkt
#lang jsonic-demo
[
 null,
 42,
 true,
 ["array", "of", "strings"],
 {
  "key-1": null,
  "key-2": false,
  "key-3": {"subkey": 21}
  }
 ]
#+END_SRC


#+BEGIN_SRC racket :tangle jsonic-demo-racket.rkt
#lang jsonic-demo
// a line comment
[
  @$ 'null $@,
  @$ (* 6 7) $@,
  @$ (= 2 (+ 1 1)) $@,
  @$ (list "array" "of" "strings") $@,
  @$ (hash 'key-1 'null
           'key-2 (even? 3)
           'key-3 (hash 'subkey 21)) $@
]
#+END_SRC

** Ready to Roll

Our language will have two main parts:
1. A reader (comprising a tokenizer and parser)
2. An expander

* Setup

#+BEGIN_SRC bash
cd path/to/jsonic
raco pkg install
#+END_SRC

* The Reader

** Booting the Reader

When we invoke #lang jsonic, Racket will look for a "main.rkt" module within our jsonic directory.
Within that module, it will expect to find a reader submodule that provides our read-syntax function.

#+BEGIN_SRC racket :tangle main.rkt
#lang br/quicklang
(module reader br
  (require "reader.rkt")
  (provide read-syntax))
#+END_SRC

** The Reader

A recap:

The reader converts the source code of our language from a string of characters into Racket-style parenthesized forms, also known as sexprs

By convention, Racket expects the name of the main reader function to be read-syntax. This read-syntax function must return one value: code for a module expression, packaged as a syntax object. This syntax object must have no bindings. In bf, we made a read-syntax function by relying on a tokenizer and parser to do the heavy lifting.

#+BEGIN_SRC racket :tangle reader.rkt
#lang br/quicklang
(require "tokenizer.rkt" "parser.rkt")

(define (read-syntax path port)
  (define parse-tree (parse path (make-tokenizer port)))
  (define module-datum `(module jsonic-module expander ,parse-tree))
  (datum->syntax #f module-datum))
(provide read-syntax)
#+END_SRC

* The Tokenizer

#+BEGIN_SRC racket :tangle tokenizer.rkt
#lang br/quicklang
(require brag/support)

(define (make-tokenizer port)
  (define (next-token)
    (define jsonic-lexer
      (lexer
       ; add rules
       [(from/to "//" "\n") (next-token)]  ; handles line comments. ignore everything from // to the new line. once we have the match, ignore it by calling next-token
       [(from/to "@$" "$@") (token 'SEXP-TOK (trim-ends "@$" lexeme "$@"))]  ; takes in sexpression tokens
       [any-char (token 'CHAR-TOK lexeme)]  ; handles tokens not processed by the above
       ))
    (jsonic-lexer port))
  next-token)
(provide make-tokenizer)
#+END_SRC

Within next-token, we use a helper function called a lexer to break down the source code into tokens.

- The lexer must be able to process every token that might appear in the source, including eof
- The lexer consists of a series of branches, each representing a lexing rule. On the left side of the branch is a pattern
  that works like a regular expression. On the right is a token-creating expression
- Each time next-token is called, jsonic-lexer will read as many characters from the input port as it can while still matching
  a rule pattern
- The lexer rule will convert the matched characters (known as the lexeme) into a token using the expression on the right
- This token will be returned as the result. The process repeats until the lexer gets the eof signal

We can even do some repl testing:

First run the tokenizer (F5 in block)

#+BEGIN_SRC racket
(apply-tokenizer-maker make-tokenizer "// comment\n")
(apply-tokenizer-maker make-tokenizer "@$ (+ 6 7) $@")
(apply-tokenizer-maker make-tokenizer "hi")
#+END_SRC

* The Parser

The parser will take the tokens generated by the tokenizer and organize them into a parse tree. Later, this parse tree will be passed to the expander for further processing.

#+BEGIN_SRC racket :tangle parser.rkt
#lang brag
jsonic-program : (jsonic-char | jsonic-sexp)*
jsonic-char    : CHAR-TOK
jsonic-sexp    : SEXP-TOK
#+END_SRC

#+BEGIN_SRC racket :tangle jsonic-parser-test.rkt
#lang br
(require jsonic/parser jsonic/tokenizer brag/support)

; test comment
(parse-to-datum (apply-tokenizer-maker make-tokenizer "// line comment\n"))

; a program with a single sexp between delimiters
(parse-to-datum (apply-tokenizer-maker make-tokenizer "@$ 42 $@"))

; a program without nested delimiters
(parse-to-datum (apply-tokenizer-maker make-tokenizer "hi"))

; a program that contains all of the above
(parse-to-datum (apply-tokenizer-maker make-tokenizer "hi\n// comment\n@$ 42 $@"))
#+END_SRC

** Here Strings

A here string is introduced with #<<LABEL, where LABEL is an arbitrary name that will terminate the here string.

#+BEGIN_SRC racket :tangle jsonic-parser-test.rkt
; try a multiline program with a here string
(parse-to-datum (apply-tokenizer-maker make-tokenizer #<<GOGOGO
"foo"
//comment
@$ 42 $@
GOGOGO
                                       ))
#+END_SRC

* The Expander

The expander determines how the code produced by the reader corresponds to real Racket expressions, which are then evaluated to produce a result.
It works by adding bindings to identifiers in the code.
