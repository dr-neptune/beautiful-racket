#+TITLE: Extend a Data Format: jsonic

A JSON contains one of 6 values:
1. a null
2. a Boolean
3. a number
4. a string
5. an array
6. an object

We wish to bring automation features into JSON. This hybrid will be a domain specific language based on JSON.

* Specification

** Ground Rules

1. Every jsonic program produces valid JSON.
2. Every valid JSON file is a valid jsonic program.
3. Racket expressions can be embedded anywhere within a jsonic program that a JSON value would appear
4. If the characters // appear in a line, the rest of the line will be commented out

#+BEGIN_SRC racket :tangle jsonic-demo-json.rkt
#lang jsonic-demo
[
 null,
 42,
 true,
 ["array", "of", "strings"],
 {
  "key-1": null,
  "key-2": false,
  "key-3": {"subkey": 21}
  }
 ]
#+END_SRC


#+BEGIN_SRC racket :tangle jsonic-demo-racket.rkt
#lang jsonic-demo
// a line comment
[
  @$ 'null $@,
  @$ (* 6 7) $@,
  @$ (= 2 (+ 1 1)) $@,
  @$ (list "array" "of" "strings") $@,
  @$ (hash 'key-1 'null
           'key-2 (even? 3)
           'key-3 (hash 'subkey 21)) $@
]
#+END_SRC

** Ready to Roll

Our language will have two main parts:
1. A reader (comprising a tokenizer and parser)
2. An expander

* Setup

#+BEGIN_SRC bash
cd path/to/jsonic
raco pkg install
#+END_SRC

* The Reader

** Booting the Reader

When we invoke #lang jsonic, Racket will look for a "main.rkt" module within our jsonic directory.
Within that module, it will expect to find a reader submodule that provides our read-syntax function.

#+BEGIN_SRC racket :tangle main.rkt
#lang br/quicklang
(module reader br
  (require "reader.rkt")
  (provide read-syntax))
#+END_SRC

** The Reader

A recap:

The reader converts the source code of our language from a string of characters into Racket-style parenthesized forms, also known as sexprs

By convention, Racket expects the name of the main reader function to be read-syntax. This read-syntax function must return one value: code for a module expression, packaged as a syntax object. This syntax object must have no bindings. In bf, we made a read-syntax function by relying on a tokenizer and parser to do the heavy lifting.

#+BEGIN_SRC racket :tangle reader.rkt
#lang br/quicklang
(require "tokenizer.rkt" "parser.rkt")

(define (read-syntax path port)
  (define parse-tree (parse path (make-tokenizer port)))
  (define module-datum `(module jsonic-module jsonic/expander ,parse-tree))
  (datum->syntax #f module-datum))
(provide read-syntax)
#+END_SRC

* The Tokenizer

#+BEGIN_SRC racket :tangle tokenizer.rkt
#lang br/quicklang
(require brag/support)

(define (make-tokenizer port)
  (define (next-token)
    (define jsonic-lexer
      (lexer
       ; add rules
       [(from/to "//" "\n") (next-token)]  ; handles line comments. ignore everything from // to the new line. once we have the match, ignore it by calling next-token
       [(from/to "@$" "$@") (token 'SEXP-TOK (trim-ends "@$" lexeme "$@"))]  ; takes in sexpression tokens
       [any-char (token 'CHAR-TOK lexeme)]  ; handles tokens not processed by the above
       ))
    (jsonic-lexer port))
  next-token)
(provide make-tokenizer)
#+END_SRC

Within next-token, we use a helper function called a lexer to break down the source code into tokens.

- The lexer must be able to process every token that might appear in the source, including eof
- The lexer consists of a series of branches, each representing a lexing rule. On the left side of the branch is a pattern
  that works like a regular expression. On the right is a token-creating expression
- Each time next-token is called, jsonic-lexer will read as many characters from the input port as it can while still matching
  a rule pattern
- The lexer rule will convert the matched characters (known as the lexeme) into a token using the expression on the right
- This token will be returned as the result. The process repeats until the lexer gets the eof signal

We can even do some repl testing:

First run the tokenizer (F5 in block)

#+BEGIN_SRC racket
(apply-tokenizer-maker make-tokenizer "// comment\n")
(apply-tokenizer-maker make-tokenizer "@$ (+ 6 7) $@")
(apply-tokenizer-maker make-tokenizer "hi")
#+END_SRC

* The Parser

The parser will take the tokens generated by the tokenizer and organize them into a parse tree. Later, this parse tree will be passed to the expander for further processing.

#+BEGIN_SRC racket :tangle parser.rkt
#lang brag
jsonic-program : (jsonic-char | jsonic-sexp)*
jsonic-char    : CHAR-TOK
jsonic-sexp    : SEXP-TOK
#+END_SRC

#+BEGIN_SRC racket :tangle jsonic-parser-test.rkt
#lang br
(require jsonic/parser jsonic/tokenizer brag/support)

; test comment
(parse-to-datum (apply-tokenizer-maker make-tokenizer "// line comment\n"))

; a program with a single sexp between delimiters
(parse-to-datum (apply-tokenizer-maker make-tokenizer "@$ 42 $@"))

; a program without nested delimiters
(parse-to-datum (apply-tokenizer-maker make-tokenizer "hi"))

; a program that contains all of the above
(parse-to-datum (apply-tokenizer-maker make-tokenizer "hi\n// comment\n@$ 42 $@"))
#+END_SRC

** Here Strings

A here string is introduced with #<<LABEL, where LABEL is an arbitrary name that will terminate the here string.

#+BEGIN_SRC racket :tangle jsonic-parser-test.rkt
; try a multiline program with a here string
(parse-to-datum (apply-tokenizer-maker make-tokenizer #<<GOGOGO
"foo"
//comment
@$ 42 $@
GOGOGO
                                       ))
#+END_SRC

* The Expander

The expander determines how the code produced by the reader corresponds to real Racket expressions, which are then evaluated to produce a result.
It works by adding bindings to identifiers in the code. The main job of the expander is to bind the identifiers in a parse tree, even though no define expressions
appear in the parse tree itself. This is why the reader delivers code without bindings, so the expander can start with a blank slate. The expander does this
job by exporting a binding (using provide) for each identifier in the parse tree.

** Designing the Expander

Racket starts the expander for a language by invoking a macro called #%module-begin.

We also just saw how the parse tree produced by the jsonic parser follows the production rules of the grammar:

#+BEGIN_SRC racket
'(jsonic-program
  (jsonic-char "\"")
  (jsonic-char "f")
  (jsonic-char "o")
  (jsonic-char "o")
  (jsonic-char "\"")
  (jsonic-char "\n")
  (jsonic-sexp " 42 "))
#+END_SRC

In turn, we can use these production rules to organize the rest of our expander.

1. Each production rule in the grammar will have a corresponding macro or function in the expander
2. The name (on the left side) of each rule is the name of the corresponding macro or function
3. The pattern (on the right side) of each rule describes the possible input to that corresponding macro or function

We should have 3 transformers:
- jsonic-program, that accepts any # of arguments
- jsonic-char that accepts 1 argument, which is the string value from a CHAR-TOK token
- jsonic-sexp that accepts 1 argument, which is the string value from a SEXP-TOK token

** Starting the Expander

#+BEGIN_SRC racket :tangle expander.rkt
#lang br/quicklang
(require json)

(define-macro (jsonic-mb PARSE-TREE)
  #'(#%module-begin
     ; validate our new string as valid JSON, and return the result
     (define result-string PARSE-TREE)
     (define validated-jsexpr (string->jsexpr result-string))
     (display result-string)))
(provide (rename-out [jsonic-mb #%module-begin]))
#+END_SRC

** Expanding the Parse Tree

Now we'll add the macros that correspond to the production rules in our jsonic grammar

#+BEGIN_SRC racket :tangle expander.rkt
(define-macro (jsonic-char CHAR-TOK-VALUE)
  #'CHAR-TOK-VALUE)
(provide jsonic-char)

(define-macro (jsonic-program SEXP-OR-JSON-STR ...)
  #'(string-trim (string-append SEXP-OR-JSON-STR ...)))
(provide jsonic-program)

(define-macro (jsonic-sexp SEXP-STR)
  (with-pattern ([SEXP-DATUM (format-datum '~a #'SEXP-STR)])
    #'(jsexpr->string SEXP-DATUM)))
(provide jsonic-sexp)
#+END_SRC

* Testing the Language

#+BEGIN_SRC racket :tangle jsonic-lang-test.rkt
#lang jsonic

; test passing valid json returns the same json
[
 null,
 42,
 true,
 ["array", "of", "strings"],
 {
  "key-1": null,
  "key-2": false,
  "key-3": {"subkey": 21}
 }
 ]

; test invalid json fails (3/5 not allowed)
[
 null,
 3/5,
 true,
 ["array", "of", "strings"],
 {
  "key-1": null,
  "key-2": false,
  "key-3": {"subkey": 21}
 }
 ]

; test replacing json with racket sexprs
// a line comment
[
 @$ 'null $@,
 @$ (* 6 7) $@,
 @$ (= 2 (+ 1 1)) $@,
 @$ (list "array" "of" "strings") $@,
 @$ (hash 'key-1 'null
          'key-2 (even? 3)
          'key-3 (hash 'subkey 21)) $@

]

; test racket sexprs fail when translating to invalid json
// a line comment
[
 @$ 'null $@,
 @$ (/ 3 5) $@,
 @$ (= 2 (+ 1 1)) $@,
 @$ (list "array" "of" "strings") $@,
 @$ (hash 'key-1 'null
          'key-2 (even? 3)
          'key-3 (hash 'subkey 21)) $@

]
#+END_SRC

* Contracts

A contract wraps around a function and ensures its input arguments and return value meet requirements that we specify. If the function tries to accept arguments or return a value that doesn't meet these requirements, the contract raises an error.

#+BEGIN_SRC racket :tangle contract-test.rkt
#lang racket
(module our-submod br
  (require racket/contract)
  (define (our-div num denom)
    (/ num denom))
  ; contract-out lets us attach a contract to an exported function
  (provide (contract-out
            ; (number? (not/c zero?) . -> . number?) means input must be a number not zero and output must be a number
            [out-div (number? (not/c zero?) . -> . number?)])))

(require (submod "." our-submod))
(our-div 42 0)
#+END_SRC
